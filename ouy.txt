diff --git a/configs/config.yaml b/configs/config.yaml
index 2820a8e..5fe3649 100644
--- a/configs/config.yaml
+++ b/configs/config.yaml
@@ -5,13 +5,13 @@ defaults:
   - _self_
 
 # General settings
-task: right_left
+task: free_instructed
 seed: 42
 device: cuda
 
 # Data settings
 data:
-  data_dir: ./data/09_25_7
+  data_dir: ../eeg-classification/data/raw/09_25_256hz
   batch_size: 32
   val_split: 0.2
 
diff --git a/configs/preprocessing/default.yaml b/configs/preprocessing/default.yaml
index f84542d..9739382 100644
--- a/configs/preprocessing/default.yaml
+++ b/configs/preprocessing/default.yaml
@@ -1,4 +1,4 @@
-enabled: false
+enabled: true
 filter_simple_trials: false
 filter_incongruent: false
 
@@ -19,4 +19,4 @@ truncate:
   end_timepoint: null
 
 handle_nans: true
-normalize_per_trial: false
+normalize_per_trial: true
diff --git a/configs/training/default.yaml b/configs/training/default.yaml
index b677db6..9cc5177 100644
--- a/configs/training/default.yaml
+++ b/configs/training/default.yaml
@@ -2,3 +2,17 @@ epochs: 100
 patience: 15
 learning_rate: 0.0001
 weight_decay: 0.00001
+
+# Learning rate scheduler
+scheduler:
+  enabled: true
+  type: ReduceLROnPlateau  # Options: ReduceLROnPlateau, StepLR, CosineAnnealingLR
+  # ReduceLROnPlateau parameters
+  factor: 0.5
+  patience: 5
+  # StepLR parameters
+  step_size: 30
+  gamma: 0.1
+  # CosineAnnealingLR parameters
+  T_max: 100
+  eta_min: 1e-6
diff --git a/src/training/trainer.py b/src/training/trainer.py
index fa0ed99..3cf3885 100644
--- a/src/training/trainer.py
+++ b/src/training/trainer.py
@@ -16,9 +16,38 @@ class Trainer:
         self.optimizer = torch.optim.Adam(
             model.parameters(),
             lr=config.learning_rate,
-            weight_decay=config.weight_decay
+            weight_decay=config.get('weight_decay', 0.0)
         )
 
+        # Learning rate scheduler
+        scheduler_config = getattr(config, 'scheduler', None)
+        if scheduler_config is not None and scheduler_config.get('enabled', False):
+            scheduler_type = scheduler_config.get('type', 'ReduceLROnPlateau')
+            if scheduler_type == 'ReduceLROnPlateau':
+                self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
+                    self.optimizer,
+                    mode='max',  # maximize validation accuracy
+                    factor=scheduler_config.get('factor', 0.5),
+                    patience=scheduler_config.get('patience', 5),
+                    verbose=True
+                )
+            elif scheduler_type == 'StepLR':
+                self.scheduler = torch.optim.lr_scheduler.StepLR(
+                    self.optimizer,
+                    step_size=scheduler_config.get('step_size', 30),
+                    gamma=scheduler_config.get('gamma', 0.1)
+                )
+            elif scheduler_type == 'CosineAnnealingLR':
+                self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
+                    self.optimizer,
+                    T_max=scheduler_config.get('T_max', config.epochs),
+                    eta_min=scheduler_config.get('eta_min', 1e-6)
+                )
+            else:
+                raise ValueError(f"Unknown scheduler type: {scheduler_type}")
+        else:
+            self.scheduler = None
+
         self.best_val_acc = 0.0
         self.patience_counter = 0
 
@@ -95,6 +124,9 @@ class Trainer:
             train_loss, train_metrics = self.train_epoch()
             val_loss, val_metrics = self.validate()
 
+            # Get current learning rate
+            current_lr = self.optimizer.param_groups[0]['lr']
+
             wandb.log({
                 'epoch': epoch,
                 'train_loss': train_loss,
@@ -103,11 +135,20 @@ class Trainer:
                 'val_loss': val_loss,
                 'val_accuracy': val_metrics['accuracy'],
                 'val_f1': val_metrics['f1_score'],
+                'learning_rate': current_lr,
             })
 
             print(f"Epoch {epoch+1}/{self.config.epochs}")
             print(f"  Train Loss: {train_loss:.4f} | Train Acc: {train_metrics['accuracy']:.4f}")
             print(f"  Val Loss: {val_loss:.4f} | Val Acc: {val_metrics['accuracy']:.4f}")
+            print(f"  Learning Rate: {current_lr:.6f}")
+
+            # Update learning rate scheduler
+            if self.scheduler is not None:
+                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
+                    self.scheduler.step(val_metrics['accuracy'])
+                else:
+                    self.scheduler.step()
 
             if val_metrics['accuracy'] > self.best_val_acc:
                 self.best_val_acc = val_metrics['accuracy']
diff --git a/src/utils/preprocessing.py b/src/utils/preprocessing.py
index 3f8a8d2..f13e11a 100644
--- a/src/utils/preprocessing.py
+++ b/src/utils/preprocessing.py
@@ -1,4 +1,7 @@
 import numpy as np
+import logging
+
+logger = logging.getLogger(__name__)
 
 
 class TrialFilter:
@@ -13,11 +16,19 @@ class TrialFilter:
         mask = np.zeros(len(primes), dtype=bool)
         for p, c, r in simple_patterns:
             mask |= (primes == p) & (cues == c) & (reactions == r)
+
+        n_kept = np.sum(mask)
+        n_total = len(primes)
+        logger.info(f"Filter simple trials: kept {n_kept}/{n_total} trials ({100*n_kept/n_total:.1f}%)")
         return mask
 
     @staticmethod
     def filter_incongruent(primes, cues):
-        return primes != cues
+        mask = primes != cues
+        n_kept = np.sum(mask)
+        n_total = len(primes)
+        logger.info(f"Filter incongruent: kept {n_kept}/{n_total} trials ({100*n_kept/n_total:.1f}%)")
+        return mask
 
     @staticmethod
     def filter_by_trial_length(trial_lengths, min_length=None, max_length=None):
@@ -26,6 +37,10 @@ class TrialFilter:
             mask &= trial_lengths >= min_length
         if max_length is not None:
             mask &= trial_lengths <= max_length
+
+        n_kept = np.sum(mask)
+        n_total = len(trial_lengths)
+        logger.info(f"Filter by length (min={min_length}, max={max_length}): kept {n_kept}/{n_total} trials ({100*n_kept/n_total:.1f}%)")
         return mask
 
 
@@ -33,6 +48,7 @@ class ChannelSelector:
     @staticmethod
     def select_channels(electrode_names, channel_names):
         channel_indices = []
+        missing_channels = []
         # Convert all names to uppercase for case-insensitive matching
         electrode_names_upper = np.array([e.upper().strip('.') for e in electrode_names])
 
@@ -41,6 +57,15 @@ class ChannelSelector:
             matches = np.where(electrode_names_upper == name_upper)[0]
             if len(matches) > 0:
                 channel_indices.append(matches[0])
+            else:
+                missing_channels.append(name)
+
+        n_selected = len(channel_indices)
+        n_requested = len(channel_names)
+        logger.info(f"Channel selection: selected {n_selected}/{n_requested} channels")
+        if missing_channels:
+            logger.warning(f"Missing channels not found in data: {missing_channels}")
+
         return np.array(channel_indices)
 
 
@@ -50,6 +75,7 @@ class TrialTransformer:
         locked_trials = np.full_like(trials, np.nan)
         n_channels, n_trials, n_timepoints = trials.shape
 
+        valid_trials = 0
         for trial_idx in range(n_trials):
             rt = int(response_times[trial_idx])
             if rt <= 0 or rt >= n_timepoints:
@@ -57,7 +83,9 @@ class TrialTransformer:
 
             trial_data = trials[:, trial_idx, :rt]
             locked_trials[:, trial_idx, -rt:] = trial_data
+            valid_trials += 1
 
+        logger.info(f"Response locking: locked {valid_trials}/{n_trials} trials with valid response times")
         return locked_trials
 
     @staticmethod
@@ -65,7 +93,11 @@ class TrialTransformer:
         n_channels, n_trials, n_timepoints = trials.shape
         start_idx = start_timepoint if start_timepoint is not None else 0
         end_idx = end_timepoint if end_timepoint is not None else n_timepoints
-        return trials[:, :, start_idx:end_idx]
+
+        truncated = trials[:, :, start_idx:end_idx]
+        new_length = truncated.shape[2]
+        logger.info(f"Truncate trials: {n_timepoints} -> {new_length} timepoints (start={start_idx}, end={end_idx})")
+        return truncated
 
     @staticmethod
     def handle_nans(trials, fill_value=0.0):
@@ -85,6 +117,7 @@ class TrialTransformer:
             mean[np.isnan(mean)] = 0.0
             normalized[:, trial_idx, :] = (trial_data - mean) / std
 
+        logger.info(f"Normalize per trial: normalized {n_trials} trials (per-channel z-score)")
         return normalized
 
 
diff --git a/train.py b/train.py
index cdb1b0f..b2ddb59 100644
--- a/train.py
+++ b/train.py
@@ -82,15 +82,9 @@ def main(cfg: DictConfig):
 
     device = cfg.device if torch.cuda.is_available() else 'cpu'
 
-    class TrainingConfig:
-        def __init__(self, cfg):
-            self.task = cfg.task
-            self.epochs = cfg.training.epochs
-            self.patience = cfg.training.patience
-            self.learning_rate = cfg.training.learning_rate
-            self.weight_decay = cfg.training.weight_decay
-
-    training_config = TrainingConfig(cfg)
+    # Pass the training config and task name directly
+    training_config = cfg.training
+    training_config.task = cfg.task
 
     trainer = Trainer(
         model=model,
commit abbdef6a52519c67ad85822d7a2e06747db582ed
Author: gilas19 <gilad.ticher@gmail.com>
Date:   Thu Nov 6 10:48:31 2025 +0200

    Add EEGPT support and reorganize config structure
    
    Major changes:
    - Integrate EEGPTClassifier model from EEGPT.py
    - Add model selection in train.py (CNN or EEGPT)
    - Reorganize configs into modular subdirectories:
      - configs/model/ for model-specific settings
      - configs/preprocessing/ for preprocessing pipelines
      - configs/training/ for training hyperparameters
    - Update preprocessing to use timepoints instead of ratios for truncation
    - Add case-insensitive electrode name matching in channel selector
    - Create EEGPT config with frontal/central electrode selection
    - Add _self_ to all config defaults to fix Hydra warnings
    - Update all tests to match new preprocessing API
    
    Config structure:
    - Main config.yaml uses Hydra defaults system
    - Modular configs can be mixed and matched
    - EEGPT config uses 32 frontal/central electrodes
    - All task configs updated with proper defaults
    
    ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)
    
    Co-Authored-By: Claude <noreply@anthropic.com>

commit 6e1a81b2fe02d1cef1e0073e7ec68c90ab048703
Author: gilas19 <gilad.ticher@gmail.com>
Date:   Thu Nov 6 09:56:28 2025 +0200

    Add comprehensive preprocessing system with tests
    
    - Implement modular preprocessing pipeline with TrialFilter, ChannelSelector, and TrialTransformer classes
    - Add filtering for simple trials (RRR, LLL, NRR, NLL patterns), incongruent trials, and trial length
    - Add channel selection by electrode name
    - Implement response locking for aligning trials to response time
    - Add trial truncation with configurable start/end ratios
    - Add NaN handling and per-trial normalization
    - Simplify config structure: consolidate subdirectories into single config.yaml
    - Create task-specific config files that inherit from base config
    - Add comprehensive test suite for all preprocessing functions
    - Update requirements.txt and train.py to support preprocessing
    
    ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)
    
    Co-Authored-By: Claude <noreply@anthropic.com>

commit e868200d465d1351033cd739f04317254ee00932
Author: gilas19 <gilad.ticher@gmail.com>
Date:   Thu Nov 6 09:23:58 2025 +0200

    Initial commit: EEG classification project
    
    - PyTorch-based EEG classification for 3 tasks
    - CNN model with GELU activation
    - Hydra configuration system
    - WandB integration
    - Subject-wise data splitting
    - Gradient clipping and proper initialization
    
    ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)
    
    Co-Authored-By: Claude <noreply@anthropic.com>
