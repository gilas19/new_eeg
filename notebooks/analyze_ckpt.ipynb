{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "228b0650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "path = \"/Users/giladticher/Documents/eeg-classification/checkpoints/eegpt_mcae_58chs_4s_large4E.ckpt\"\n",
    "ckpt = torch.load(path, map_location=torch.device(\"cpu\"), weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92b781a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_encoder.summary_token torch.Size([1, 4, 512])\n",
      "target_encoder.patch_embed.proj.weight torch.Size([512, 1, 1, 64])\n",
      "target_encoder.patch_embed.proj.bias torch.Size([512])\n",
      "target_encoder.chan_embed.weight torch.Size([62, 512])\n",
      "target_encoder.blocks.0.norm1.weight torch.Size([512])\n",
      "target_encoder.blocks.0.norm1.bias torch.Size([512])\n",
      "target_encoder.blocks.0.attn.qkv.weight torch.Size([1536, 512])\n",
      "target_encoder.blocks.0.attn.qkv.bias torch.Size([1536])\n",
      "target_encoder.blocks.0.attn.proj.weight torch.Size([512, 512])\n",
      "target_encoder.blocks.0.attn.proj.bias torch.Size([512])\n",
      "target_encoder.blocks.0.norm2.weight torch.Size([512])\n",
      "target_encoder.blocks.0.norm2.bias torch.Size([512])\n",
      "target_encoder.blocks.0.mlp.fc1.weight torch.Size([2048, 512])\n",
      "target_encoder.blocks.0.mlp.fc1.bias torch.Size([2048])\n",
      "target_encoder.blocks.0.mlp.fc2.weight torch.Size([512, 2048])\n",
      "target_encoder.blocks.0.mlp.fc2.bias torch.Size([512])\n",
      "target_encoder.blocks.1.norm1.weight torch.Size([512])\n",
      "target_encoder.blocks.1.norm1.bias torch.Size([512])\n",
      "target_encoder.blocks.1.attn.qkv.weight torch.Size([1536, 512])\n",
      "target_encoder.blocks.1.attn.qkv.bias torch.Size([1536])\n",
      "target_encoder.blocks.1.attn.proj.weight torch.Size([512, 512])\n",
      "target_encoder.blocks.1.attn.proj.bias torch.Size([512])\n",
      "target_encoder.blocks.1.norm2.weight torch.Size([512])\n",
      "target_encoder.blocks.1.norm2.bias torch.Size([512])\n",
      "target_encoder.blocks.1.mlp.fc1.weight torch.Size([2048, 512])\n",
      "target_encoder.blocks.1.mlp.fc1.bias torch.Size([2048])\n",
      "target_encoder.blocks.1.mlp.fc2.weight torch.Size([512, 2048])\n",
      "target_encoder.blocks.1.mlp.fc2.bias torch.Size([512])\n",
      "target_encoder.blocks.2.norm1.weight torch.Size([512])\n",
      "target_encoder.blocks.2.norm1.bias torch.Size([512])\n",
      "target_encoder.blocks.2.attn.qkv.weight torch.Size([1536, 512])\n",
      "target_encoder.blocks.2.attn.qkv.bias torch.Size([1536])\n",
      "target_encoder.blocks.2.attn.proj.weight torch.Size([512, 512])\n",
      "target_encoder.blocks.2.attn.proj.bias torch.Size([512])\n",
      "target_encoder.blocks.2.norm2.weight torch.Size([512])\n",
      "target_encoder.blocks.2.norm2.bias torch.Size([512])\n",
      "target_encoder.blocks.2.mlp.fc1.weight torch.Size([2048, 512])\n",
      "target_encoder.blocks.2.mlp.fc1.bias torch.Size([2048])\n",
      "target_encoder.blocks.2.mlp.fc2.weight torch.Size([512, 2048])\n",
      "target_encoder.blocks.2.mlp.fc2.bias torch.Size([512])\n",
      "target_encoder.blocks.3.norm1.weight torch.Size([512])\n",
      "target_encoder.blocks.3.norm1.bias torch.Size([512])\n",
      "target_encoder.blocks.3.attn.qkv.weight torch.Size([1536, 512])\n",
      "target_encoder.blocks.3.attn.qkv.bias torch.Size([1536])\n",
      "target_encoder.blocks.3.attn.proj.weight torch.Size([512, 512])\n",
      "target_encoder.blocks.3.attn.proj.bias torch.Size([512])\n",
      "target_encoder.blocks.3.norm2.weight torch.Size([512])\n",
      "target_encoder.blocks.3.norm2.bias torch.Size([512])\n",
      "target_encoder.blocks.3.mlp.fc1.weight torch.Size([2048, 512])\n",
      "target_encoder.blocks.3.mlp.fc1.bias torch.Size([2048])\n",
      "target_encoder.blocks.3.mlp.fc2.weight torch.Size([512, 2048])\n",
      "target_encoder.blocks.3.mlp.fc2.bias torch.Size([512])\n",
      "target_encoder.blocks.4.norm1.weight torch.Size([512])\n",
      "target_encoder.blocks.4.norm1.bias torch.Size([512])\n",
      "target_encoder.blocks.4.attn.qkv.weight torch.Size([1536, 512])\n",
      "target_encoder.blocks.4.attn.qkv.bias torch.Size([1536])\n",
      "target_encoder.blocks.4.attn.proj.weight torch.Size([512, 512])\n",
      "target_encoder.blocks.4.attn.proj.bias torch.Size([512])\n",
      "target_encoder.blocks.4.norm2.weight torch.Size([512])\n",
      "target_encoder.blocks.4.norm2.bias torch.Size([512])\n",
      "target_encoder.blocks.4.mlp.fc1.weight torch.Size([2048, 512])\n",
      "target_encoder.blocks.4.mlp.fc1.bias torch.Size([2048])\n",
      "target_encoder.blocks.4.mlp.fc2.weight torch.Size([512, 2048])\n",
      "target_encoder.blocks.4.mlp.fc2.bias torch.Size([512])\n",
      "target_encoder.blocks.5.norm1.weight torch.Size([512])\n",
      "target_encoder.blocks.5.norm1.bias torch.Size([512])\n",
      "target_encoder.blocks.5.attn.qkv.weight torch.Size([1536, 512])\n",
      "target_encoder.blocks.5.attn.qkv.bias torch.Size([1536])\n",
      "target_encoder.blocks.5.attn.proj.weight torch.Size([512, 512])\n",
      "target_encoder.blocks.5.attn.proj.bias torch.Size([512])\n",
      "target_encoder.blocks.5.norm2.weight torch.Size([512])\n",
      "target_encoder.blocks.5.norm2.bias torch.Size([512])\n",
      "target_encoder.blocks.5.mlp.fc1.weight torch.Size([2048, 512])\n",
      "target_encoder.blocks.5.mlp.fc1.bias torch.Size([2048])\n",
      "target_encoder.blocks.5.mlp.fc2.weight torch.Size([512, 2048])\n",
      "target_encoder.blocks.5.mlp.fc2.bias torch.Size([512])\n",
      "target_encoder.blocks.6.norm1.weight torch.Size([512])\n",
      "target_encoder.blocks.6.norm1.bias torch.Size([512])\n",
      "target_encoder.blocks.6.attn.qkv.weight torch.Size([1536, 512])\n",
      "target_encoder.blocks.6.attn.qkv.bias torch.Size([1536])\n",
      "target_encoder.blocks.6.attn.proj.weight torch.Size([512, 512])\n",
      "target_encoder.blocks.6.attn.proj.bias torch.Size([512])\n",
      "target_encoder.blocks.6.norm2.weight torch.Size([512])\n",
      "target_encoder.blocks.6.norm2.bias torch.Size([512])\n",
      "target_encoder.blocks.6.mlp.fc1.weight torch.Size([2048, 512])\n",
      "target_encoder.blocks.6.mlp.fc1.bias torch.Size([2048])\n",
      "target_encoder.blocks.6.mlp.fc2.weight torch.Size([512, 2048])\n",
      "target_encoder.blocks.6.mlp.fc2.bias torch.Size([512])\n",
      "target_encoder.blocks.7.norm1.weight torch.Size([512])\n",
      "target_encoder.blocks.7.norm1.bias torch.Size([512])\n",
      "target_encoder.blocks.7.attn.qkv.weight torch.Size([1536, 512])\n",
      "target_encoder.blocks.7.attn.qkv.bias torch.Size([1536])\n",
      "target_encoder.blocks.7.attn.proj.weight torch.Size([512, 512])\n",
      "target_encoder.blocks.7.attn.proj.bias torch.Size([512])\n",
      "target_encoder.blocks.7.norm2.weight torch.Size([512])\n",
      "target_encoder.blocks.7.norm2.bias torch.Size([512])\n",
      "target_encoder.blocks.7.mlp.fc1.weight torch.Size([2048, 512])\n",
      "target_encoder.blocks.7.mlp.fc1.bias torch.Size([2048])\n",
      "target_encoder.blocks.7.mlp.fc2.weight torch.Size([512, 2048])\n",
      "target_encoder.blocks.7.mlp.fc2.bias torch.Size([512])\n",
      "target_encoder.norm.weight torch.Size([512])\n",
      "target_encoder.norm.bias torch.Size([512])\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "params = []\n",
    "for k, v in ckpt[\"state_dict\"].items():\n",
    "    if k.startswith(\"target_encoder.\"):\n",
    "        print(k, v.shape)\n",
    "        params.append((k, v.shape))\n",
    "\n",
    "print(len(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fead7ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([\n",
    "                            'Fp1', 'FpZ', 'Fp2', \n",
    "                        \"AF7\",'AF3', 'AF4', \"AF8\",\n",
    "            'F7', 'F5', 'F3', 'F1', 'Fz', 'F2', 'F4', 'F6', 'F8', \n",
    "        'FT7', 'FC5', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'FC6', 'FT8', \n",
    "            'T7', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6', 'T8', \n",
    "        'TP7', 'CP5', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'CP6', 'TP8',\n",
    "             'P7', 'P5', 'P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8', \n",
    "                'PO7', \"PO5\", 'PO3', 'POz',  'PO4', \"PO6\", 'PO8', \n",
    "                               'O1', 'Oz', 'O2'\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b71582e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([\n",
    "                            'Fp1', 'FpZ', 'Fp2', \n",
    "                               'AF3', 'AF4', \n",
    "            'F7', 'F5', 'F3', 'F1', 'Fz', 'F2', 'F4', 'F6', 'F8', \n",
    "        'FT7', 'FC5', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'FC6', 'FT8', \n",
    "            'T7', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6', 'T8', \n",
    "        'TP7', 'CP5', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'CP6', 'TP8',\n",
    "             'P7', 'P5', 'P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8', \n",
    "                      'PO7', 'PO3', 'POz',  'PO4', 'PO8', \n",
    "                               'O1', 'Oz', 'O2'\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b95c69e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(['Fp1', 'FpZ', 'Fp2', \n",
    "                               'AF3', 'AF4', \n",
    "            'F7', 'F5', 'F3', 'F1', 'Fz', 'F2', 'F4', 'F6', 'F8', \n",
    "        'FT7', 'FC5', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'FC6', 'FT8', \n",
    "            'T7', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6', 'T8', \n",
    "        'TP7', 'CP5', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'CP6', 'TP8',\n",
    "             'P7', 'P5', 'P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8', \n",
    "                      'PO7', 'PO3', 'POz',  'PO4', 'PO8', \n",
    "                               'O1', 'Oz', 'O2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3d6fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML-DEEP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
